---
quickshare-date: 2024-04-16 23:14:41
quickshare-url: "https://noteshare.space/note/clv2gtioj1562201mwebmagg6j#zy6n6T/PlfbXyLGOcv++txRGlfFL4sVUDDSx5Ox5tcY"
---
# 4장 : 가장 훌륭한 예측선

- 딥러닝을 이해하기 위해서 알아야 하는 두 가지
	- **선형 회귀**
	- **로지스틱 회귀**
- 가장 훌륭한 예측선 : 선형 회귀를 의미
## 선형 회귀의 정의

- **독립 변수** : 독립적으로 변할 수 있는 값 (ex: 성적을 변하게 하는 *정보*)
- **종속 변수** : 독립 변수에 따라 종속적으로 변화하는 값 (ex: *성적*)
- **선형 회귀** : 독립 변수를 이용해 종속 변수의 움직임을 예측하고 설명하는 작업

- **단순 선형 회귀** : 하나의 x값으로 y값을 설명할 수 있을 때
- **다중 선형 회귀** : x값이 여러 개 필요할 때

## 가장 훌륭한 예측선이란?

- 만약 주어진 데이터가 선형을 보인다면 -> 이 특징을 잘 나타내는 일차 함수 그래프를 그릴 수 있음
$$y = ax+b$$
- $x$ : 독립 변수, $y$ : 종속 변수 -> $x$값에 따라 $y$값이 변화함.
- but 이를 정확히 예측하기 위해서는 정확한 a, b값이 필요
- 정확한 a, b값을 구해낸다면?
	- <mark style="background: #FFF3A3A6;">주어지지 않은 다른 학생의 성적을 예측</mark>하는 것이 가능!
- 따라서 선형 회귀의 개념을 이해하는 것은 <mark style="background: #FFF3A3A6;">딥러닝을 이해하는 데 중요한 첫걸음</mark>

## 최소 제곱법

- **최소 제곱법**을 통해 일차 함수의 기울기 a와 b를 바로 구하는 것이 가능.
- 공식은 다음과 같음 :
$$a = \frac{(x-x평균)(y-y평균)의합}{(x-x평균)^2의합}$$
> 분자 : x의 편차의 제곱의 합
> 분모 : x와 y의 편차를 곱해 합한 값 

$$b = y의 평균 - (x의 평균 * 기울기 a)$$
- 이를 통해 만든 직선에 값을 대입함으로써 결과를 예측할 수 있음

## 평균 제곱 오차

- 선을 그을 때 가장 많이 사용하는 방법 : 일단 그리고 조금씩 수정해 나가기
- 이 과정에서 선의 오차를 계산하는 방법이 필요
- 가장 많이 사용되는 오차 평가 방법 : **평균 제곱 오차**
- 만약 오차를 제곱 없이 그대로 더할 시 부호가 서로 다르므로 값이 0이 될 수 있음
	- -> *제곱해서 더해주기!*
- **평균 제곱 오차(MSE)** : 오차의 합을 원소의 개수로 나눈 것
$$
평균 \,제곱 \, 오차(MSE) = \frac{1}{n}\sum(y_i-\hat y_i)^2
$$
